#!/usr/bin/env python3
# ov_data_imagenet2backbone.py

# chmod +x ov_data_imagenet2backbone.py
# ./ov_data_imagenet2backbone.py \
#   --source-root /mnt/image-net-full/junha/dataset/OneVisionData \
#   --target-root /mnt/backbone-nfs/junha/dataset/OneVisionData \
#   --cache-dir /mnt/image-net-full/junha/.cache/huggingface

import os
import glob
import argparse
import json
import csv
from io import BytesIO

from datasets import load_dataset, Image as HFImage
from tqdm import tqdm
from PIL import Image, UnidentifiedImageError

def convert_dataset(source_root: str, target_base: str, cache_dir: str):
    """
    source_root: ì›ë³¸ parquet í´ë” (e.g. /mnt/image-net-full/.../OneVisionData/ai2d(cauldron,llava_format))
    target_base: ë³€í™˜ëœ ê²°ê³¼ê°€ ì €ì¥ë  ìµœìƒìœ„ ë””ë ‰í† ë¦¬ 
                 (e.g. /mnt/backbone-nfs/junha/dataset/OneVisionData)
    cache_dir:   HuggingFace Dataset ìºì‹œ ë””ë ‰í† ë¦¬
    """
    base_name   = os.path.basename(source_root.rstrip("/"))
    target_root = os.path.join(target_base, base_name)
    image_dir   = os.path.join(target_root, "image")
    os.makedirs(image_dir, exist_ok=True)

    # parquet íŒŒì¼ ê²€ìƒ‰ (í´ë” ì§ì† ë˜ëŠ” ì¬ê·€)
    parquet_files = sorted(glob.glob(os.path.join(source_root, "*.parquet")))
    if not parquet_files:
        parquet_files = sorted(glob.glob(os.path.join(source_root, "**", "*.parquet"), recursive=True))
    if not parquet_files:
        print(f"[SKIP] '{base_name}'ì— parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"[START] Converting '{base_name}', {len(parquet_files)} filesâ€¦")

    # â”€â”€ 1) ìë™ ë””ì½”ë”© ë„ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ds = (
        load_dataset(
            "parquet",
            data_files={"train": parquet_files},
            split="train",
            cache_dir=cache_dir,
        )
        .cast_column("image", HFImage(decode=False))  # decode=False!  ğŸ”‘
    )

    converted, bad_samples = [], []

    for row in tqdm(ds, desc=f"  â†’ {base_name}"):
        rec = {
            "id": row["id"],
            "conversations": row.get("conversations", []),
        }

        # ì´ë¯¸ì§€ ì²˜ë¦¬ (lazyâ€‘decode & ì˜¤ë¥˜ ì²˜ë¦¬)
        bytes_ = row["image"].get("bytes") if row.get("image") else None
        if bytes_:
            try:
                img = Image.open(BytesIO(bytes_))
                img.load()  # ì‹¤ì œ ë””ì½”ë”© âœ ì˜¤ë¥˜ê°€ ì—¬ê¸°ì„œ ë°œìƒí•˜ë©´ except ë¡œ

                # JPEGì— ì“¸ ìˆ˜ ì—†ëŠ” ëª¨ë“œ(RGBA ë“±)ëŠ” RGB ë³€í™˜
                if getattr(img, "mode", "RGB") != "RGB":
                    img = img.convert("RGB")

                img_fname = f"{row['id']}.jpg"
                rec["image"] = os.path.join("OneVisionData", base_name, "image", img_fname)
                img.save(os.path.join(image_dir, img_fname))

            except (UnidentifiedImageError, OSError) as e:
                bad_samples.append({"id": row["id"], "reason": str(e)})
                continue  # í•´ë‹¹ ìƒ˜í”Œì€ ê±´ë„ˆëœ€

        converted.append(rec)

    # â”€â”€ 2) JSON íŒŒì¼ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # CSVì— ì •ì˜ëœ í´ë”â†’JSON ì´ë¦„ ë§¤í•‘ ë¡œë“œ
    json_mapping = {}
    mapping_csv = os.path.join(os.path.dirname(__file__), "OneVisionData", "JSON_Mapping.csv")
    if os.path.exists(mapping_csv):
        with open(mapping_csv, newline="", encoding="utf-8") as f:
            for row in csv.DictReader(f):
                json_mapping[row["folder_name"]] = row["json_file"]

    json_name = json_mapping.get(base_name, f"{base_name}.json")
    json_path = os.path.join(target_root, json_name)
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(converted, f, ensure_ascii=False, indent=2)

    # â”€â”€ 3) ê¹¨ì§„ ìƒ˜í”Œ ë¡œê·¸ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if bad_samples:
        log_path = os.path.join(target_root, f"{base_name}_bad_samples.csv")
        with open(log_path, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=["id", "reason"])
            writer.writeheader()
            writer.writerows(bad_samples)
        print(f"[WARN] {len(bad_samples):,} bad samples logged to {log_path}")

    print(f"[DONE] '{base_name}': {len(converted):,} good samples â†’ {json_path}")


def main():
    parser = argparse.ArgumentParser(
        description="OneVisionData ì „ì²´ ì„œë¸Œí´ë”ë¥¼ json+image í¬ë§·ìœ¼ë¡œ ì¼ê´„ ë³€í™˜í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--source-root",
        default="/mnt/image-net-full/junha/dataset/OneVisionData",
        help="ì›ë³¸ OneVisionData í´ë” ê²½ë¡œ",
    )
    parser.add_argument(
        "--target-root",
        default="/mnt/backbone-nfs/junha/dataset/OneVisionData",
        help="ë³€í™˜ ê²°ê³¼ë¥¼ ì €ì¥í•  í´ë” ê²½ë¡œ",
    )
    parser.add_argument(
        "--cache-dir",
        default="/mnt/image-net-full/junha/.cache/huggingface",
        help="HuggingFace Dataset ìºì‹œ ë””ë ‰í† ë¦¬",
    )
    args = parser.parse_args()

    # íƒ€ê²Ÿ ë£¨íŠ¸ ë§Œë“¤ê¸°
    os.makedirs(args.target_root, exist_ok=True)

    for entry in sorted(os.listdir(args.source_root)):
        src_path = os.path.join(args.source_root, entry)
        # .cache ê°™ì€ í´ë”ëŠ” ìŠ¤í‚µ
        if entry.startswith(".") or not os.path.isdir(src_path):
            continue
        # ì´ë¯¸ ë³€í™˜ëœ í´ë” ìŠ¤í‚µ
        target_folder = os.path.join(args.target_root, entry)
        if os.path.exists(target_folder):
            print(f"[SKIP] '{entry}' already exists in target directory")
            continue

        convert_dataset(src_path, args.target_root, args.cache_dir)


if __name__ == "__main__":
    main()
